# ü§ñ Sistema RAG - Consultas sobre Empresa J√∫nior

Um sistema de **Retrieval-Augmented Generation (RAG)** desenvolvido em Python para consultas inteligentes sobre diretrizes e documentos da **Comp J√∫nior** e do **Movimento Empresa J√∫nior**. O projeto foi feito com base no v√≠deo [Python RAG Tutorial (with Local LLMs): AI For Your PDFs](https://www.youtube.com/watch?v=2TJxpyO3ei4)

## üìã Sobre o Projeto

Este projeto implementa um sistema RAG que permite fazer perguntas em linguagem natural sobre documentos relacionados √† Empresa J√∫nior. O sistema:

- üìÑ Carrega e processa documentos PDF automaticamente
- üß† Utiliza embeddings locais com **Ollama** (modelo `nomic-embed-text`)
- üíæ Armazena conhecimento em base vetorial **ChromaDB**
- üîç Realiza busca sem√¢ntica para encontrar contexto relevante
- üí¨ Gera respostas contextualizadas usando **Mistral** via Ollama

### üéØ Funcionalidades Principais

- **Ingest√£o de Documentos**: Processa PDFs e divide em chunks otimizados
- **Busca Sem√¢ntica**: Encontra informa√ß√µes relevantes baseada na similaridade
- **Respostas Contextuais**: Gera respostas fundamentadas nos documentos
- **Detec√ß√£o de Conflitos**: Identifica a√ß√µes que v√£o contra diretrizes
- **Fontes Transparentes**: Mostra quais documentos foram utilizados

## üìÅ Estrutura do Projeto

```
teste_rag/
‚îú‚îÄ‚îÄ populate_db.py      # Script para popular a base de dados
‚îú‚îÄ‚îÄ query_data.py       # Script para consultas RAG
‚îú‚îÄ‚îÄ files/              # Diret√≥rio com documentos PDF
‚îÇ   ‚îú‚îÄ‚îÄ arquivo.pdf     # Documentos de normas da empresa e do movimento
‚îú‚îÄ‚îÄ chroma_db/          # Base de dados vetorial (gerada automaticamente)
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ requirements.txt    # Arquvos com as depenencias do venv
‚îî‚îÄ‚îÄ README.md
```

## üöÄ Como Executar

### Pr√©-requisitos

1. **Python 3.8+** instalado
2. **Ollama** instalado e rodando localmente
3. Modelos Ollama necess√°rios:
   ```bash
   ollama pull nomic-embed-text
   ollama pull mistral
   ```

### üì¶ Instala√ß√£o

1. **Clone o reposit√≥rio:**
   ```bash
   git clone https://github.com/MatheusPiassiC/estudos-rag.git
   cd estudos-rag
   ```

2. **Crie um ambiente virtual:**
   ```bash
   python -m venv .venv
   .venv\Scripts\activate  # Windows
   # source .venv/bin/activate  # Linux/Mac
   ```

3. **Instale as depend√™ncias:**
   ```bash
   pip install -r requirements.txt
   ```

### üîß Configura√ß√£o

1. **Ajuste os caminhos** nos arquivos Python se necess√°rio:
   - `CHROMA_PATH` em `populate_db.py` e `query_data.py`
   - Caminho dos documentos em `populate_db.py`

2. **Adicione seus PDFs** na pasta `files/`

### ‚ñ∂Ô∏è Execu√ß√£o

#### 1. Popular a Base de Dados
```bash
python populate_db.py
```
Este comando:
- Carrega todos os PDFs da pasta `files/`
- Divide os documentos em chunks de 800 caracteres
- Gera embeddings usando `nomic-embed-text`
- Armazena na base vetorial ChromaDB

#### 2. Fazer Consultas
```bash
python query_data.py "Sua pergunta aqui"
```

**Exemplos de perguntas:**
```bash
python query_data.py "Um membro da diretoria ter relacionamento amoroso com um trainee, isso √© permitido?"

python query_data.py "Quais s√£o as responsabilidades do diretor de projetos?"

python query_data.py "O que diz o c√≥digo de √©tica sobre conflitos de interesse?"
```

## üõ†Ô∏è Tecnologias Utilizadas

- **[LangChain](https://langchain.com/)**: Framework para aplica√ß√µes LLM
- **[ChromaDB](https://www.trychroma.com/)**: Base de dados vetorial
- **[Ollama](https://ollama.ai/)**: Execu√ß√£o local de LLMs
- **PyPDF**: Processamento de documentos PDF
- **Python 3.8+**: Linguagem de programa√ß√£o

## üìä Como Funciona

### 1. **Processamento de Documentos** (`populate_db.py`)
```mermaid
graph LR
A[PDFs] --> B[Carregamento]
B --> C[Divis√£o em Chunks]
C --> D[Gera√ß√£o de Embeddings]
D --> E[Armazenamento ChromaDB]
```

### 2. **Sistema de Consulta** (`query_data.py`)
```mermaid
graph LR
A[Pergunta] --> B[Embedding da Pergunta]
B --> C[Busca Similaridade]
C --> D[Recupera√ß√£o Contexto]
D --> E[Gera√ß√£o Resposta LLM]
E --> F[Resposta + Fontes]
```

## ‚öôÔ∏è Configura√ß√µes Avan√ßadas

### Ajustar Par√¢metros de Chunk
No arquivo `populate_db.py`:
```python
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=800,        # Tamanho do chunk
    chunk_overlap=80,      # Sobreposi√ß√£o entre chunks
    length_function=len,
)
```

### Modificar N√∫mero de Resultados
No arquivo `query_data.py`:
```python
results = db.similarity_search_with_score(query_text, k=5)  # Altere k para mais/menos resultados
```

### Personalizar o Prompt
Edite `PROMPT_TEMPLATE` em `query_data.py` para diferentes comportamentos:
```python
PROMPT_TEMPLATE = """
Seu prompt customizado aqui...
{context}
{question}
"""
```

## üìû Suporte

Se voc√™ encontrar problemas ou tiver d√∫vidas:

1. **Verifique se o Ollama est√° rodando**: `ollama list`
2. **Confirme os modelos instalados**: `ollama pull nomic-embed-text` e `ollama pull mistral`
3. **Verifique os caminhos dos arquivos** nos scripts Python

---

**Desenvolvido com ‚ù§Ô∏è para facilitar consultas sobre diretrizes da Empresa J√∫nior**